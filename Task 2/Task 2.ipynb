{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP236424wKcnCFeZavpJL+Q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "Implement Activation Functions in Neural Networks and analyse their usage.\n",
        "\n",
        "# Program"
      ],
      "metadata": {
        "id": "zNB0oz-4nXQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iDRYOYHqnQM7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x)) #changed from 1| to 1/ to apply sigmoid function correctly\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "def leaky_relu(x,alpha=0.01):\n",
        "  return np.where(x>0,x,alpha*x)\n",
        "def softmax(x):\n",
        "  if x.ndim == 1:\n",
        "    x = x.reshape(1, -1)\n",
        "  exp_x=np.exp(x-np.max(x,axis=1,keepdims=True))\n",
        "  return exp_x/np.sum(exp_x,axis=1,keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage\n",
        "x=np.array([-1.0,0.0,1.0,2.0])\n",
        "print(\"Sigmoid: \",sigmoid(x))\n",
        "print(\"Tanh: \",tanh(x))\n",
        "print(\"Relu: \",relu(x))\n",
        "print(\"Leaky Relu: \",leaky_relu(x))\n",
        "print(\"Softmax: \",softmax(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEeWfmbMoz-w",
        "outputId": "c1bf5ac4-26ce-4380-9e2a-6b4d7854ee3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid:  [0.26894142 0.5        0.73105858 0.88079708]\n",
            "Tanh:  [-0.76159416  0.          0.76159416  0.96402758]\n",
            "Relu:  [0. 0. 1. 2.]\n",
            "Leaky Relu:  [-0.01  0.    1.    2.  ]\n",
            "Softmax:  [[0.0320586  0.08714432 0.23688282 0.64391426]]\n"
          ]
        }
      ]
    }
  ]
}